{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyN2miGhSpPQGCwQo6gzmwY5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahoangnhatphi/AIN501C/blob/main/2_Fibonacci_macine_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fibonacci sequence"
      ],
      "metadata": {
        "id": "AhzOU75JTC-M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "# Tạo dãy Fibonacci ban đầu\n",
        "fib_sequence = [1, 1]\n",
        "for i in range(2, 100):\n",
        "    next_number = fib_sequence[-1] + 2 * fib_sequence[-2]\n",
        "    fib_sequence.append(next_number)\n",
        "for i, value in enumerate(fib_sequence, start=1):\n",
        "    print(f\"x{i-1} = {value}\")\n",
        "\n",
        "print('Predicted number: ', fib_sequence[-1] + 2 * fib_sequence[-2])"
      ],
      "metadata": {
        "id": "xXlsKzQTTb9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear Regression"
      ],
      "metadata": {
        "id": "BRzbzVwJhtz2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Lập trình qui luật và tạo dữ liệu\n",
        "def generate_sequence(n):\n",
        "    seq = [1, 1]\n",
        "    for i in range(2, n):\n",
        "        seq.append(sequence[i - 1] + 2 * sequence[i - 2])\n",
        "    return seq\n",
        "\n",
        "# Thử nghiệm với 100 số đầu tiên\n",
        "n = 100\n",
        "seq = generate_sequence(n)\n",
        "\n",
        "# Chuẩn bị dữ liệu cho Linear Regression\n",
        "X = np.array(range(1, n+1)).reshape(-1, 1)\n",
        "y = np.array(seq).reshape(-1, 1)\n",
        "\n",
        "# Tạo và huấn luyện mô hình Linear Regression\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Dự đoán giá trị cho số thứ 101\n",
        "predicted_number_value = model.predict(np.array([[101]]))\n",
        "\n",
        "print(\"Predicted number 101: \", predicted_number_value[0][0])"
      ],
      "metadata": {
        "id": "NCAvJ72shytV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM (Long Short-Term Memory)"
      ],
      "metadata": {
        "id": "4-hde0iIXzvT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Generate Fibonacci sequence\n",
        "def generate_sequence(n):\n",
        "    sequence = [1, 1]\n",
        "    for i in range(2, n):\n",
        "        sequence.append(sequence[i - 1] + 2 * sequence[i - 2])\n",
        "    return sequence\n",
        "\n",
        "n = 101\n",
        "sequence = generate_sequence(n)\n",
        "\n",
        "# Generate Sequence:\n",
        "X = np.array(sequence[:-1]).reshape(-1, 1)\n",
        "y = np.array(sequence[1:]).reshape(-1, 1)\n",
        "\n",
        "#  Use MinMaxScaler to normalize the input and target data, scaling them to a range between 0 and 1.\n",
        "scaler = MinMaxScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "y = scaler.fit_transform(y)\n",
        "\n",
        "# The data is reshaped to match the expected input shape for an LSTM model, which is (batch_size, timesteps, features). In your case, you have a single feature, so the shape becomes (batch_size, 1, 1).\n",
        "X = X.reshape(X.shape[0], 1, X.shape[1])\n",
        "y = y.reshape(y.shape[0], 1, y.shape[1])\n",
        "\n",
        "# Model has one LSTM layer with 50 units and a ReLU activation function. The output layer is a Dense layer with a single unit\n",
        "model = Sequential()\n",
        "model.add(LSTM(units=50, activation='relu', input_shape=(1, 1)))\n",
        "model.add(Dense(units=1))\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Compile Adam optimizer and mean squared error (MSE) loss function\n",
        "model.fit(X, y, epochs=100, batch_size=8, verbose=1)\n",
        "\n",
        "# Make prediction for number 101th\n",
        "predict_number = np.array([sequence[100]]).reshape(-1, 1)\n",
        "predict_number = scaler.transform(predict_number)\n",
        "predict_number = predict_number.reshape(1, 1, 1)\n",
        "predict_number_result = model.predict(predict_number)\n",
        "predict_number_result = scaler.inverse_transform(predict_number_result)\n",
        "\n",
        "print(\"Predicted number 101:\", predict_number_result)"
      ],
      "metadata": {
        "id": "qC6eqx6vT6G2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neutral network"
      ],
      "metadata": {
        "id": "TIVe6Vu8X17L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Generate Fibonacci sequence\n",
        "def generate_sequence(n):\n",
        "    sequence = [1, 1]\n",
        "    for i in range(2, n):\n",
        "        sequence.append(sequence[i - 1] + 2 * sequence[i - 2])\n",
        "    return sequence\n",
        "\n",
        "n = 101\n",
        "sequence = generate_sequence(n)\n",
        "\n",
        "# Generate Sequence:\n",
        "X = np.array(sequence[:-1]).reshape(-1, 1)\n",
        "y = np.array(sequence[1:]).reshape(-1, 1)\n",
        "\n",
        "#  Use MinMaxScaler to normalize the input and target data, scaling them to a range between 0 and 1.\n",
        "scaler = MinMaxScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "y = scaler.fit_transform(y)\n",
        "\n",
        "# The data is reshaped to match the expected input shape for an LSTM model, which is (batch_size, timesteps, features). In your case, you have a single feature, so the shape becomes (batch_size, 1, 1).\n",
        "X = X.reshape(X.shape[0], 1, X.shape[1])\n",
        "y = y.reshape(y.shape[0], 1, y.shape[1])\n",
        "\n",
        "# This neural network model with two hidden layers, ReLU activation functions, and mean squared error loss\n",
        "model = Sequential()\n",
        "model.add(Dense(units=64, activation='relu', input_dim=1))\n",
        "model.add(Dense(units=32, activation='relu'))\n",
        "model.add(Dense(units=1))\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Compile Adam optimizer and mean squared error (MSE) loss function\n",
        "model.fit(X, y, epochs=100, batch_size=8, verbose=1)\n",
        "\n",
        "# Make prediction for number 101th\n",
        "predict_number = np.array([sequence[100]]).reshape(-1, 1)\n",
        "predict_number = scaler.transform(predict_number)\n",
        "predict_number = predict_number.reshape(1, 1, 1)\n",
        "predict_number_result = model.predict(predict_number)\n",
        "predict_number_result = scaler.inverse_transform(predict_number_result)\n",
        "\n",
        "print(\"Predicted number 101:\", predict_number_result)"
      ],
      "metadata": {
        "id": "9xCqRZ-aYHAL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion"
      ],
      "metadata": {
        "id": "1jAuX6MjjXCs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Number 101th: 8.45e+29\n",
        "\n",
        "| Model                  | Result      |\n",
        "| -----------            | ----------- |\n",
        "| Linear Regression      | 3.32e+28    |\n",
        "| LSTM                   | 8.69e+29    |\n",
        "| Neural network         | 8.45e+29    |\n",
        "\n",
        "- The sequence exhibits rapid growth, which can potentially lead to scalability challenges when employing certain models.\n",
        "\n",
        "- While Linear Regression aims to predict the next number based on the current one, it may not be the optimal choice in this scenario due to the non-linear nature of the pattern.\n",
        "\n",
        "- To enhance the model's accuracy, more complex machine learning approaches such as Neural Networks, LSTM can be employed. In this case, we explore prediction using LSTM and Neural Networks.\n",
        "\n",
        "- It's evident that the model's performance progressively improves in the order of Neural Network > LSTM > Linear Regression, with the Neural Network providing the closest approximation\n"
      ],
      "metadata": {
        "id": "yoSFAee6jgVs"
      }
    }
  ]
}